---
title: "Multi-Level Linguistic Alignment in a Dynamic CPS Tasks: Step 3 Model 1"
author: "Nick Duran"
date: 10/14/21
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
```

## Notebook contains code for replicating section in manuscript: Analysis 1: Alignment over time

#### Step 1: Import main data file and run script to prep variables for analysis

```{r, warning=FALSE, message=FALSE, error=FALSE}
library(tidyverse)
library(pander)
```

```{r}
main.data = read.csv("Step1_PrepareFeatures.csv")

source("Step3_helper.R")
main.data.20 = variable_prep(main.data)

pander(names(main.data.20))
# head(main.data.20)
```

* Relies heavily on "Step3_helper.R." This code does the following:

  * remove levels that are fewer than 20 turns
  * Convert # of turns to a running proportion score 
  * Get z-scores for relevant variables
  * create ordered factor for block (interested in linear trend)
  * create random factor for within subject variance (making sure each unique subject is signified given their school and team)

#### Step 2: Check the distributions of the main outcome linguistic variables

```{r, warning=FALSE, message=FALSE, error=FALSE}
library(DataExplorer)

g1 = main.data.20 %>% select(lexical, semantic, syntax)
plot_histogram(g1)
```

The outcome variables of lexical and syntactic is clearly distribute with clumping at zero (hurdle, bounded) with a skew in the positive values (unbounded, gamma), As such, what is required is the use of zero-inflated gamma models, i.e., gamma hurdle models; zero-altered gamma models; two-part models (a binomial model for predicting occurrence of nonzero, a second [linear model (or Gamma, or truncated Normal, or log-Normal)] for evaluating the relationship of nonzero value to predictors)

#### Step 3: Build zero-inflated gamma models: LEXICAL

```{r, warning=FALSE, message=FALSE, error=FALSE}
library(glmmTMB)
library(parameters) 
```

How to interpret (generally): 

For the **linear component (gamma)**, we use a log link that allows us to predict the model-adjusted mean of the non-zero data on the log scale. Being on the log scale, we can also exponentiate to make claims about how a unit increase in response value x leads to a probabilistic decrease or increase in the mean of the outcome. For example: A 1 unit increase in x leads to a 0.012 decrease in the mean of the outcome. Put another way, increasing x by 1 causes a 98.8% reduction in the predictor. 

For the **binomial component** we use a logit link to predict "0" (nonoccurrence) and then exponentiate the resulting coefficient to report the probability of seeing a nonoccurrence. 


```{r}
zigamma_lex <- glmmTMB(lexical ~ ordering_prop + Block.l + aligner_role + relative_start_timeZ + level_durationZ + utterlen_alignerZ + Concept + (1|subject_id),
                         family = ziGamma(link = "log"),
                         ziformula = ~ ordering_prop + Block.l + aligner_role + relative_start_timeZ + level_durationZ + utterlen_alignerZ + Concept + (1|subject_id), 
                         data = main.data.20)
summary(zigamma_lex)
```

Let's do the exponentiation of the coefficients for clearer interpretation

```{r}
model_parameters(zigamma_lex, exponentiate = T)
```

Show the model parameters in a cleaner format

```{r, warning=FALSE, message=FALSE, error=FALSE}
library(strengejacke)
```

```{r, warning=FALSE, message=FALSE, error=FALSE}
tab_model(zigamma_lex
          # show.intercept = FALSE,
          # show.r2 = TRUE,
          # show.icc = FALSE,
          # show.re.var = FALSE,
          # show.ngroups = FALSE,
          # show.obs = FALSE,
          # string.est = "Incidence Rate Ratios"
          )
```

How to interpret (specifically): 

<!-- For interpreting the intercept, the mean value (model adjusted on log-scale) of lexical is 0.26 -->

For the **linear component (gamma)** component, assuming a non-zero value, a 1 unit increase in ordering_prop (from the start [0] to the end [1]) leads to a 0.97 decrease in the mean of lexical. Put in another way, increasing ordering_prop 0 to 1 (start to finish) causes a 2.93% (1-(exp(-0.0297724))) * 100 reduction in the mean amount of lexical alignment. Or, we can change the unit size to .10 increments. If so, increasing ordering_prop by 1/10 increments causes a 0.30% (1-exp(-0.0297724 * 0.1)) * 100 reduction in the mean amount of lexical.

For the **binomial component** predicting whether lexical alignment occurred at all, although no change of likelihood across a level, for likelihood across blocks, the odds of there being no lexical alignment increased by 1.07 times exp(0.0686873). That is, there was an 7.11% increase in having no occurrence of lexical alignment by the final round.

#### Step 4: Build zero-inflated gamma models: SYNTACTIC (without lexical boost)

```{r}
zigamma_syn <- glmmTMB(syntax ~ ordering_prop + Block.l + aligner_role + relative_start_timeZ + level_durationZ + utterlen_alignerZ + Concept + (1|subject_id),
                         family = ziGamma(link = "log"), 
                         ziformula = ~ ordering_prop + Block.l + aligner_role + relative_start_timeZ + level_durationZ + utterlen_alignerZ + Concept + (1|subject_id), 
                         data = main.data.20)

summary(zigamma_syn)
```

```{r}
model_parameters(zigamma_syn, exponentiate = T)
```

```{r, warning=FALSE, message=FALSE, error=FALSE}
tab_model(zigamma_syn)
```

<!-- For interpreting the intercept, the mean value (model adjusted on log-scale) of syntax is 0.11 -->

For the **linear component (gamma)** component, assuming a non-zero value, a 1 unit increase in ordering_prop (from the start [0] to the end [1]) leads to a 0.92 decrease in the mean of syntax. Put in another way, increasing ordering_prop 0 to 1 (start to finish) causes a 7.52% (1-(exp(-0.078183))) * 100 reduction in the mean amount of syntactic alignment. Or, we can change the unit size to .10 increments. If so, increasing ordering_prop by 1/10 increments causes a 0.78% (1-exp(-0.078183 * 0.1)) * 100 reduction in the mean amount of syntax.

For the **binomial component** predicting whether syntactic alignment occurred at all, although no change of likelihood across a level, for likelihood across blocks, the odds of there being no syntactic alignment increased by 1.06 times exp(0.055121). That is, there was an 5.67% increase in having no occurrence of syntactic alignment by the final round.

#### Step 5: Table to be used for paper

```{r, warning=FALSE, message=FALSE, error=FALSE}
tab_model(zigamma_lex, zigamma_syn,
          show.ci = 0.95,
          rm.terms = c("Block.l.Q"),
          show.intercept = FALSE,
          pred.labels = c("ordering_prop"="Utterance order", "Block.l.L"="Block (linear trend)", "BlockExpBlock2"="Block 2 (vs Warmup)", "aligner_rolecontroller"="Role [controller]", "relative_start_timeZ"="Level start time", "level_durationZ"="Level duration", "utterlen_alignerZ"="Utterance length", "ConceptPoT" = "Concept [PoT]", "Revisited_birevisit" = "Revisit [yes]", "XQ_know"="Familiarity [yes]", "femdomin"="Majority Female [yes]"),
          # title = "",
          digits = 3,
          digits.re = 3,
          dv.labels = c("Lexical", "Syntactic"),
          show.r2 = FALSE,
          show.icc = FALSE,
          show.re.var = FALSE,
          show.ngroups = FALSE,
          show.obs = FALSE, 
          string.est = "Incidence Rate Ratios",
          use.viewer = TRUE)
```

#### Step 6: Build mixed effect models: SEMANTIC

Semantic

```{r, warning=FALSE, message=FALSE, error=FALSE}

library(lme4)

## observations: 1709 rows were not computed because one of the utterances contained words not in the semantic space
## looks normally distributed, slightly skewed to the right
# g1 = gather(df.sl3, "lexical", "semantic", "syn2nd", key="ling_align", value="values")
# p1 = ggplot(g1, aes(values))
# p1 +  geom_histogram(data=subset(g1,ling_align == 'semantic'), binwidth = 0.05)

lmer_sem = lmer(semantic ~ ordering_prop + Block.l + aligner_role + relative_start_timeZ + level_durationZ + utterlen_alignerZ + Concept + (1|subject_id), main.data.20); 

summary(lmer_sem)

```

```{r, warning=FALSE, message=FALSE, error=FALSE}
tab_model(lmer_sem)
```

#### Step 7: Table to be used for paper

```{r, warning=FALSE, message=FALSE, error=FALSE}
tab_model(lmer_sem,
          show.ci = 0.95, #FALSE
          rm.terms = c("Block.l.Q"),
          show.intercept = FALSE,
          pred.labels = c("ordering_prop"="Utterance order", "Block.l.L"="Block (linear trend)", "BlockExpBlock2"="Block 2 (vs Warmup)", "aligner_rolecontroller"="Role [controller]", "relative_start_timeZ"="Level start time", "level_durationZ"="Level duration", "utterlen_alignerZ"="Utterance length", "ConceptPoT" = "Concept [PoT]", "Revisited_birevisit" = "Revisit [yes]", "XQ_know"="Familiarity [yes]", "femdomin"="Majority Female [yes]"),
          # title = "",
          digits = 3,
          digits.re = 3,
          dv.labels = c("Semantic"),
          show.re.var = FALSE,
          show.r2 = FALSE,
          show.icc = FALSE,
          show.ngroups = FALSE,
          show.obs = FALSE, 
          use.viewer = TRUE)

```


```{r}
# mod1 = plot_model(lmer_sem, type = "eff", terms = c("ordering_prop"))
# mod1 + labs(title="Semantic Alignment",
#         x ="Ordering", y = "Cosine")
# save_plot("semantic_time.png")

```












