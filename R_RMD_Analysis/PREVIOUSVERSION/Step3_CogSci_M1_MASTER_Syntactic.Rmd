---
title: "Multi-Level Linguistic Alignment in a Dynamic CPS Tasks: Step 3 Model 1"
author: "Nick Duran"
date: 10/14/21
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# rm(list=ls())
```

> Notebook contains code for replicating section in manuscript: Analysis 1: Alignment over time

### Step 1: Import main data file and run script to prep variables for analysis

```{r, warning=FALSE, message=FALSE, error=FALSE}
library(tidyverse)
library(pander)
```

```{r}
main.data = read.csv("../Data/Step1_PrepareFeatures.csv")

# test1 = main.data %>%
#   group_by(Team, School, Block, Level, revisited) %>%
#   filter(n() >= 20) %>%
#   ungroup()
#   
# ## remove extraordinarily long single utterances (removes 282 rows, or (282/34294)*100, or less than 1%)
# ## why 71? Because this is 5SDs above mean of utterlen: mean(main.data$utterlen_aligner)+(5*sd(main.data$utterlen_aligner))
# test2 = test1 %>% filter(utterlen_aligner < 71 & utterlen_target < 71)

source("../R_Helper/Step3_helper.R")
main.data.20 = variable_prep(main.data)

pander(names(main.data.20))
# head(main.data.20)
```

* Relies heavily on "Step3_helper.R." This code does the following:

  * remove levels that are fewer than 20 turns
  * Convert # of turns to a running proportion score 
  * Get z-scores for relevant variables
  * create ordered factor for block (interested in linear trend)
  * create random factor for within subject variance (making sure each unique subject is signified given their school and team)

### Step 2: Check the distributions of the main outcome linguistic variables

```{r, warning=FALSE, message=FALSE, error=FALSE}
library(DataExplorer)

g1 = main.data.20 %>% select(lexical, syntax, semantic)
plot_histogram(g1)
```

The outcome variables of lexical and syntactic is clearly distribute with clumping at zero (hurdle, bounded) with a skew in the positive values (unbounded, gamma), As such, what is required is the use of zero-inflated gamma models, i.e., gamma hurdle models; zero-altered gamma models; two-part models (a binomial model for predicting occurrence of nonzero, a second [linear model (or Gamma, or truncated Normal, or log-Normal)] for evaluating the relationship of nonzero value to predictors)

### Step 3: Build zero-inflated gamma models: SYNTAX (without lexical boost)

```{r, warning=FALSE, message=FALSE, error=FALSE}
library(glmmTMB)
library(parameters) 

library(strengejacke)
```

#### glmmTMB approach

To start, going to build with a glmmTMB model that allows random effect structure. We are going to use the gamma distribution because we are dealing with non-integer (non-count) data as our DV and thus is continuously distributed, with some skew in the positive direction. Starting with the simplest random effect structure model with subject. 

<!-- Going to use a ziGamma model for overdisperion, but is there a way to check for this?  -->
<!-- truncated_nbinom2 instead of ziGamma? -->

```{r}
zigamma_syn <- glmmTMB(syntax ~ ordering_prop + Block.l + aligner_role + relative_start_timeZ + level_durationZ + utterlen_alignerZ + Concept + (1|subject_id),
                         family = ziGamma(link = "log"), #gamma because deling with non-integer counts
                         ziformula = ~ ordering_prop + Block.l + aligner_role + relative_start_timeZ + level_durationZ + utterlen_alignerZ + Concept + (1|subject_id), 
                         data = main.data.20)
```

```{r}
summary(zigamma_syn)
```

The initial summary shows that subject-level variance is incredibly small. 

What about a more complex random effects structure? Would this improve variance accounted for at subject-level? Attempting two more models with theoretically-plausible structures and will compare all models with an AIC test for selection. 

[Note: I have also attempted to run models with "Level" but does not converge. And a model with "(ordering_prop|subject_id) + (Block.l|subject_id)" does not converge]. 

```{r}
zigamma_syn2 <- glmmTMB(syntax ~ ordering_prop + Block.l + aligner_role + relative_start_timeZ + level_durationZ + utterlen_alignerZ + Concept + (aligner_role|subject_id),
                         family = ziGamma(link = "log"), 
                         ziformula = ~ ordering_prop + Block.l + aligner_role + relative_start_timeZ + level_durationZ + utterlen_alignerZ + Concept + (aligner_role|subject_id),
                         data = main.data.20)
```

```{r}
summary(zigamma_syn2)
# tab_model(zigamma_syn2, digits.re = 3)
```

```{r}
## Did not converge

# zigamma_syn3 <- glmmTMB(syntax ~ ordering_prop + Block.l + aligner_role + relative_start_timeZ + level_durationZ + utterlen_alignerZ + Concept + (ordering_prop|subject_id),
#                          family = ziGamma(link = "log"),
#                          ziformula = ~ ordering_prop + Block.l + aligner_role + relative_start_timeZ + level_durationZ + utterlen_alignerZ + Concept + (ordering_prop|subject_id), 
#                          data = main.data.20)
```

```{r}
# summary(zigamma_syn3)
# tab_model(zigamma_syn3)
```

```{r}
## Did not converge

# zigamma_syn4 <- glmmTMB(syntax ~ ordering_prop + Block.l + aligner_role + relative_start_timeZ + level_durationZ + utterlen_alignerZ + Concept + (1+aligner_role|subject_id) + (0+ordering_prop|subject_id),
#                          family = ziGamma(link = "log"),
#                          ziformula = ~ ordering_prop + Block.l + aligner_role + relative_start_timeZ + level_durationZ + utterlen_alignerZ + Concept + (1+aligner_role|subject_id) + (0+ordering_prop|subject_id),
#                          data = main.data.20)
# 
```

```{r}
# summary(zigamma_syn4)
# # tab_model(zigamma_syn4)
```

To compare models, use ‘bbmle’. See: https://journal.r-project.org/archive/2017/RJ-2017-066/RJ-2017-066.pdf, page 381: use the  AICtab function. Best resources for doing a comparison is with: 
https://www.ashander.info/posts/2015/10/model-selection-glms-aic-what-to-report/
http://ms.mcmaster.ca/~bolker/R/misc/foxchapter/bolker_chap.html

What we're looking for is the highest weighted model. 

```{r}
model.names <- c("1 Subject", "2 Subject + Aligner Role")
reported.table1 <- bbmle::AICtab(zigamma_syn, zigamma_syn2, weights = TRUE, sort = FALSE, mnames = model.names)
reported.table1
```

Although a mixed effects models with a role|aligner effects structure is best, the random effect structure is still incredibly small. 

```{r}
VarCorr(zigamma_syn2)
```

#### glm and glmer approach (explicitly run two separate models)

For comparison, we can also explicitly fit (non-mixed) generalized linear models with complete pooling of subject. Here, we fit separately the conditional and zero-inflated model. We use the method outlined here: https://seananderson.ca/2014/05/18/gamma-hurdle/. 

```{r}
main.data.20.separate = main.data.20 %>% mutate(non_zero_syn = ifelse(syntax > 0, 1, 0))
```

```{r}
## zero-inflated binomial model
z1.glm <- glm(non_zero_syn ~ ordering_prop + Block.l + aligner_role + relative_start_timeZ + level_durationZ + utterlen_alignerZ + Concept, data = main.data.20.separate, family = binomial(link = logit))

## NOTE: was getting an warning that "Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred" ; but this appears to be an issue because there were  extreme outliers for utterance length of some rows. Removed these outliers (.80% of data) and now no issue with model. Used this to resolve: https://stats.stackexchange.com/questions/336424/issue-with-complete-separation-in-logistic-regression-in-r https://stats.stackexchange.com/questions/396008/glm-fit-fitted-probabilities-numerically-0-or-1-occurred-however-culprit-featur
```

```{r}
summary(z1.glm)
# tab_model(z1.glm)
```

```{r}
## linear gamma model
g1.glm <- glm(syntax ~ ordering_prop + Block.l + aligner_role + relative_start_timeZ + level_durationZ + utterlen_alignerZ + Concept, data = subset(main.data.20.separate, non_zero_syn == 1), family = Gamma(link = log))
```

```{r}
summary(g1.glm)
# tab_model(g1.glm)
```

The results are very similar, if not nearly identical. 

Is there a preferred one to use? One way is to compare models with and without random effects. To do that, need to separate out the models for the zero-inflated and gamma components as did with GLM. But now with GLMER. 

```{r}
library(lme4)
library(aods3)

z1.glmer <- glmer(non_zero_syn ~ ordering_prop + Block.l + aligner_role + relative_start_timeZ + level_durationZ + utterlen_alignerZ + Concept + (1|subject_id), data = main.data.20.separate, family = binomial(link = logit))

g1.glmer <- glmer(syntax ~ ordering_prop + Block.l + aligner_role + relative_start_timeZ + level_durationZ + utterlen_alignerZ + Concept + (1|subject_id), data = subset(main.data.20.separate, non_zero_syn == 1), family = Gamma(link = log))
```

Now compare, and it looks like the random effects structure model wins out. 

```{r}
reported.table2 <- bbmle::AICtab(z1.glm, z1.glmer, weights = TRUE, sort = FALSE)
reported.table2

reported.table3 <- bbmle::AICtab(g1.glm, g1.glmer, weights = TRUE, sort = FALSE)
reported.table3
```

We can also check for over-dispersion to help justify the gamma distribution if we wanted. The description for how to do this can be found in these resources:
http://ms.mcmaster.ca/~bolker/R/misc/foxchapter/bolker_chap.html

The problem is that the easy function for checking (aods3:gof) does not work with glmmTMB (zigamma_lex2). So, just using g1.glmer. But really, it is clearly over-dispersed and using the gamma distribution makes sense. 

```{r}
library(aods3)
```

Check the GLM model:

```{r}
gof(g1.glm)
```

Check the GLMER model:

```{r}
gof(g1.glmer)
```
### Conclusion





















Let's do the exponentiation of the coefficients for clearer interpretation

```{r}
# model_parameters(zigamma_lex, exponentiate = T)
```

Show the model parameters in a cleaner format

```{r, warning=FALSE, message=FALSE, error=FALSE}
tab_model(zigamma_lex
          # show.intercept = FALSE,
          # show.r2 = TRUE,
          # show.icc = FALSE,
          # show.re.var = FALSE,
          # show.ngroups = FALSE,
          # show.obs = FALSE,
          # string.est = "Incidence Rate Ratios"
          )
```

How to interpret (specifically): 

#### Step 4: Build zero-inflated gamma models: SYNTACTIC (without lexical boost)

```{r}
zigamma_syn <- glmmTMB(syntax ~ ordering_prop + Block.l + aligner_role + relative_start_timeZ + level_durationZ + utterlen_alignerZ + Concept + (1|subject_id),
                         family = ziGamma(link = "log"), 
                         ziformula = ~ ordering_prop + Block.l + aligner_role + relative_start_timeZ + level_durationZ + utterlen_alignerZ + Concept + (1|subject_id), 
                         data = main.data.20)

summary(zigamma_syn)
```

```{r}
model_parameters(zigamma_syn, exponentiate = T)
```

```{r, warning=FALSE, message=FALSE, error=FALSE}
tab_model(zigamma_syn)
```

<!-- For interpreting the intercept, the mean value (model adjusted on log-scale) of syntax is 0.11 -->

For the **linear component (gamma)** component, assuming a non-zero value, a 1 unit increase in ordering_prop (from the start [0] to the end [1]) leads to a 0.92 decrease in the mean of syntax. Put in another way, increasing ordering_prop 0 to 1 (start to finish) causes a 7.52% (1-(exp(-0.078183))) * 100 reduction in the mean amount of syntactic alignment. Or, we can change the unit size to .10 increments. If so, increasing ordering_prop by 1/10 increments causes a 0.78% (1-exp(-0.078183 * 0.1)) * 100 reduction in the mean amount of syntax.

For the **binomial component** predicting whether syntactic alignment occurred at all, although no change of likelihood across a level, for likelihood across blocks, the odds of there being no syntactic alignment increased by 1.06 times exp(0.055121). That is, there was an 5.67% increase in having no occurrence of syntactic alignment by the final round.





#### Step 5: Table to be used for paper

```{r, warning=FALSE, message=FALSE, error=FALSE}
tab_model(zigamma_lex, zigamma_syn,
          show.ci = 0.95,
          rm.terms = c("Block.l.Q"),
          show.intercept = FALSE,
          pred.labels = c("ordering_prop"="Utterance order", "Block.l.L"="Block (linear trend)", "BlockExpBlock2"="Block 2 (vs Warmup)", "aligner_rolecontroller"="Role [controller]", "relative_start_timeZ"="Level start time", "level_durationZ"="Level duration", "utterlen_alignerZ"="Utterance length", "ConceptPoT" = "Concept [PoT]", "Revisited_birevisit" = "Revisit [yes]", "XQ_know"="Familiarity [yes]", "femdomin"="Majority Female [yes]"),
          # title = "",
          digits = 3,
          digits.re = 3,
          dv.labels = c("Lexical", "Syntactic"),
          show.r2 = FALSE,
          show.icc = FALSE,
          show.re.var = FALSE,
          show.ngroups = FALSE,
          show.obs = FALSE, 
          string.est = "Incidence Rate Ratios",
          use.viewer = TRUE)
```







Ok, new notes from 11/19/21:

A very good paper for just talking about the random effect structure of LMER models, ensures that what is being reported is accurate and gives a critical citation: http://www.singmann.org/download/publications/singmann_kellen-introduction-mixed-models.pdf

An interesting discussion of how to determine whether the random effects variables are worth including, whether a mixed effects model is even necessary. Brings up the interesting idea of using models with and without the effect and comparing with AIC
https://stats.stackexchange.com/questions/56150/how-can-i-test-whether-a-random-effect-is-significant

In one of the responses to the main thread, a very good point that "although there is 'obviously' variation in subject performance, the extent of this subject variation can be fully or virtually-fully explained by just the residual variance term alone. There is not enough additional subject-level variation to warrant adding an additional subject-level random effect to explain all the observed variation."
https://stats.stackexchange.com/questions/115090/why-do-i-get-zero-variance-of-a-random-effect-in-my-mixed-model-despite-some-va

Not the greatest post, but makes the point that gamma works on continuous non-negative distributions, whereas poisson and negative binomial do not:
https://timothy-barry.github.io/posts/2020-06-16-gamma-poisson-nb/

********Topic: Overdispersion
Not sure whether I need to explicity test for this or not, as visually, it appears to be overdispersed. However, need to plot the residuals to know for sure. So, strike previous comment. Cannot merely plot the DV to make a determination. 
Just ok resources but useable to back up some claims. 
https://biometry.github.io/APES/LectureNotes/2016-JAGS/Overdispersion/OverdispersionJAGS.pdf
https://biometry.github.io/APES//LectureNotes/2016-JAGS/Overdispersion/OverdispersionJAGS.html

*******Topic: "How to model non-negative zero-inflated continuous data?"
Given this is exactly what I'm trying to do, this thread was helpful and pointed to the need of running zero-inflated/hurdle models (i.e., two-part models) 
https://stats.stackexchange.com/questions/187824/how-to-model-non-negative-zero-inflated-continuous-data

But this post also suggests running a model where you run the two parts yourself rather than relying on a single function, like glmmTMB, especially if I want to build a model without the random effects structure. This tutorial shows you how:
https://seananderson.ca/2014/05/18/gamma-hurdle/

This seananderson paper is really useful as it does make the points that most packages are built for count data (and he lists a bunch of these options), but this doesn't help me much. He also reinforces the point that what is needed are binomial-Gamma hurdle models. 

Use this paper when citing glmmTMB models:
https://journal.r-project.org/archive/2017/RJ-2017-066/RJ-2017-066.pdf









